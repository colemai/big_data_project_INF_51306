{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["# Spark Dataframes\n", "\n", "RDDs are the building blocks of Spark. It\u2019s the original API that Spark exposed\n", "and pretty much all the higher level APIs decompose to RDDs. The advantages of \n", "RDDs are manifold, but there are also some problems. For example, it\u2019s easy to \n", "build inefficient transformation chains, they are slow with non-JVM languages \n", "such as Python, they can not be optimized by Spark. Lastly, it\u2019s difficult to \n", "understand what is going on when you\u2019re working with them, because, for example, \n", "the transformation chains are not very readable in the sense that you don\u2019t \n", "immediately see what will be the solution, but how you are doing it.\n", "\n", "#### DataFrame\n", "\n", "Because of the disadvantages that you can experience while working with RDDs, \n", "the DataFrame API was conceived: it provides you with a higher level abstraction \n", "that allows you to use a query language to manipulate the data. This higher level \n", "abstraction is a logical plan that represents data and a schema. This means that \n", "the frontend to interacting with your data is a lot easier! Because the logical \n", "plan will be converted to a physical plan for execution, you\u2019re actually a lot \n", "closer to what you\u2019re doing when you\u2019re working with them rather than how you\u2019re \n", "trying to do it, because you let Spark figure out the most efficient way to do \n", "what you want to do.\n", "\n", "Remember though that DataFrames are still built on top of RDDs!\n", "\n", "While you still can interact directly with RDDs, **DataFrames are preferred**. They're\n", "generally faster, and they perform the same no matter what language (Python, R,\n", "Scala or Java) you use with Spark.\n", "\n", "In this set of tutorials, we'll learn how to use DataFrames, and the following\n", "transformations will be covered:\n", "\n", "- `select()`, `filter()`, `distinct()`, `dropDuplicates()`, `orderBy()`,\n", "`groupBy()`\n", "\n", "The following actions will be covered:\n", "- `first()`, `take()`, `count()`, `collect()`, `show()`\n", "\n", "Also covered:\n", "- `cache()`, `unpersist()`\n", "\n", "\n", "## Part 1: Using DataFrames and chaining together transformations and actions\n", "\n", "### Working with your first DataFrames\n", "\n", "The entry point for using data frames is the [SQLContext](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import pixiedust\n", "from pixiedust import sc\n", "pixiedust.enableJobMonitor()\n", "\n", "sqlContext = pixiedust.SQLContext(sc)"], "outputs": [], "metadata": {}}, {"source": ["In Spark, we first create a base [DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). \n", "We can then apply one ormore transformations to that base DataFrame. \n", "*A DataFrame is, just like RDDs, immutable, so once it is created, it cannot be changed.* \n", "As a result, each transformation creates a new DataFrame. \n", "Finally, we can apply one or more actions to the DataFrames.\n", "\n", "> Note again that Spark uses lazy evaluation, so transformations are not actually executed until an action occurs.\n", "\n", "We will perform several exercises to obtain a better understanding of\n", "DataFrames:\n", "\n", "* Read a file containing 10,000 personal records\n", "* Create a Spark DataFrame from that collection\n", "* Subtract one from each value using `map`\n", "* Perform action `collect` to view results\n", "* Perform action `count` to view counts\n", "* Apply transformation `filter` and view results with `collect`\n", "* Learn about lambda functions\n", "* Explore how lazy evaluation works and the debugging challenges that it\n", "introduces\n", "\n", "A DataFrame consists of a series of `Row` objects; each `Row` object has a set\n", "of named columns. \n", "You can think of a DataFrame as modeling a table, though the data source being processed does not have to be a table.\n", "\n", "More formally, a DataFrame must have a _schema_, which means it must consist of\n", "columns, each of which has a _name_ and a _type_. \n", "Some data sources have schemas built into them. \n", "Examples include RDBMS databases, Parquet files, and NoSQL\n", "databases like Cassandra. \n", "Other data sources don't have computer-readable schemas, but you can often apply a schema programmatically.\n", "\n", "# Long Example\n", "## A dataset of 10.000 random people\n", "\n", "A collection has been created consisting of random data of fake person records.\n", "This collection is available in the data folder.\n", "\n", "First, we will create a list containing tuples with this data."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Open persons.txt\n", "data = []\n", "import csv\n", "\n", "# Open persons.txt\n", "with open('../data/persons.txt') as person_file:\n", "    # Iterate through the file\n", "    for line in csv.reader(person_file, delimiter=';'):\n", "        # Convert last value (age) into int\n", "        line[-1] = int(line[-1])\n", "        # Append values to data list\n", "        data.append(tuple(line))"], "outputs": [], "metadata": {}}, {"source": ["`data` is just a normal Python list, containing Python tuples objects. Let's\n", "look at the first item in the list:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["data[0]"], "outputs": [], "metadata": {}}, {"source": ["We can check the size of the list using the Python `len()` function."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["len(data)"], "outputs": [], "metadata": {}}, {"source": ["## Create a DataFrame from a collection\n", "\n", "In Spark, datasets are represented as a list of entries, where the list is\n", "broken up into many different partitions that are each stored on a different\n", "machine.  Each partition holds a unique subset of the entries in the list.\n", "\n", "One of the defining features of Spark, compared to other data analytics\n", "frameworks (e.g., Hadoop), is that it stores data in memory rather than on disk.\n", "This allows Spark applications to run much more quickly, because they are not\n", "slowed down by needing to read data from disk.\n", "The figure below illustrates how Spark breaks a list of data entries into\n", "partitions that are each stored in memory on a worker (executor on the right hand side of the diagram).\n", "\n", "![](http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3b.png)\n", "\n", "To create the DataFrame, we'll use `sqlContext.createDataFrame()`, and we'll\n", "pass our array of data in as an argument to that function. Spark will create a\n", "new set of input data based on data that is passed in.  A DataFrame requires a\n", "_schema_, which is a list of columns, where each column has a name and a type.\n", "Our list of data has elements with types (mostly strings, but one integer).\n", "\n", "We'll supply the rest of the schema and the column names as the second argument\n", "to `createDataFrame()`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))"], "outputs": [], "metadata": {}}, {"source": ["Let's see what type `sqlContext.createDataFrame()` returned."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print('type of dataDF: %s', type(dataDF))"], "outputs": [], "metadata": {}}, {"source": ["Let's take a look at the DataFrame's schema and some of its rows."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.printSchema()"], "outputs": [], "metadata": {}}, {"source": ["On how many partitions is this DataFrame split into?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.rdd.getNumPartitions()"], "outputs": [], "metadata": {}}, {"source": ["### Use _select_ to retrieve data\n", "\n", "So far, we've created a distributed DataFrame that is split into many\n", "partitions, where each partition is stored on a single machine in our cluster.\n", "Let's look at what happens when we do a basic operation on the dataset.  Many\n", "useful data analysis operations can be specified as \"do something to each item\n", "in the dataset\".  These data-parallel operations are convenient because each\n", "item in the dataset can be processed individually: the operation on one entry\n", "doesn't effect the operations on any of the other entries.  Therefore, Spark can\n", "parallelize the operation.\n", "\n", "One of the most common DataFrame operations is `select()`, and it works more or\n", "less like a SQL `SELECT` statement: You can select specific columns from the\n", "DataFrame, and you can even use `select()` to create _new_ columns with values\n", "that are derived from existing column values. We can use `select()` to create a\n", "new column that decrements the value of the existing `age` column.\n", "\n", "Note that `select()` is a _transformation_. It returns a new DataFrame that captures both\n", "the previous DataFrame and the operation to add to the query (`select`, in this\n", "case). \n", "But it does *not* actually execute anything on the cluster. When\n", "transforming DataFrames, we are building up a _query plan_. That query plan will\n", "be optimized, implemented (in terms of RDDs), and executed by Spark _only_ when\n", "we call an action."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age'\n", "# Because select is a transformation and Spark uses lazy evaluation, no jobs, stages,\n", "# or tasks will be launched when we run this code.\n", "subDF = dataDF.select('last_name', 'first_name', 'ssn', 'occupation', (dataDF.age - 1).alias('age'))"], "outputs": [], "metadata": {}}, {"source": ["This transformation is lazy! (of course!). i.e. nothing has been computed yet.\n", "Let's take a look at `subDF`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["subDF"], "outputs": [], "metadata": {}}, {"source": ["### Use _collect_ to view results\n", "\n", "![Collect](http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3d.png)\n", "\n", "To see a list of elements decremented by one, we need to create a new list on\n", "the driver from the the data distributed in the executor nodes.  To do this we\n", "can call the `collect()` method on our DataFrame.  `collect()` is often used\n", "after transformations to ensure that we are only returning a *small* amount of\n", "data to the driver.  This is done because the data returned to the driver must\n", "fit into the driver's available memory.  If not, the driver will crash.\n", "\n", "The `collect()` method is the first action operation that we have encountered.\n", "Action operations cause Spark to perform the (lazy) transformation operations\n", "that are required to compute the values returned by the action.  In our example,\n", "this means that tasks will now be launched to perform the `createDataFrame`,\n", "`select`, and `collect` operations.\n", "\n", "In the diagram, the dataset is broken into four partitions, so four `collect()`\n", "tasks are launched. Each task collects the entries in its partition and sends\n", "the result to the driver, which creates a list of the values, as shown in the\n", "figure below.\n", "\n", "Now let's run `collect()` on `subDF`.\n", "\n", "Check via the job scheduler, that this task is executed in 10 stages - as many as the partitions of this dataframe!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Let's collect the data\n", "results = subDF.collect()\n", "print(results)"], "outputs": [], "metadata": {}}, {"source": ["A better way to visualize the data is to use the `show()` method. If you don't\n", "tell `show()` how many rows to display, it displays 20 rows."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["subDF.show()"], "outputs": [], "metadata": {}}, {"source": ["### Use _count_ to get total\n", "\n", "One of the most basic jobs that we can run is the `count()` job which will count\n", "the number of elements in a DataFrame, using the `count()` action. Since\n", "`select()` creates a new DataFrame with the same number of elements as the\n", "starting DataFrame, we expect that applying `count()` to each DataFrame will\n", "return the same result.\n", "\n", "\n", "\n", "Note that because `count()` is an action operation, if we had not already\n", "performed an action with `collect()`, then Spark would now perform the\n", "transformation operations when we executed `count()`.\n", "\n", "The figure below, shows how it works.\n", "\n", "![Count](http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3e.png)\n", "\n", "Each task counts the entries in its partition and sends the result to your\n", "SparkContext, which adds up all of the counts. The figure above shows\n", "what would happen if we ran `count()` on a small example dataset with just four\n", "partitions."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(dataDF.count())\n", "print(subDF.count())"], "outputs": [], "metadata": {}}, {"source": ["### Apply transformation _filter_ and view results with _collect_\n", "\n", "Next, we'll create a new DataFrame that only contains the people whose ages are\n", "less than 10. To do this, we'll use the `filter()` transformation. (You can also\n", "use `where()`, an alias for `filter()`, if you prefer something more SQL-like).\n", "The `filter()` method is a transformation operation that creates a new DataFrame\n", "from the input DataFrame, keeping only values that match the filter expression.\n", "\n", "The figure shows how this might work on the small four-partition dataset.\n", "\n", "![Filter](http://spark-mooc.github.io/web-assets/images/cs105x/diagram-3f.png)\n", "\n", "\n", "To view the filtered list of elements less than 10, we need to create a new list\n", "on the driver from the distributed data on the executor nodes.  We use the\n", "`collect()` method to return a list that contains all of the elements in this\n", "filtered DataFrame to the driver program."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["filteredDF = subDF.filter(subDF.age < 10)\n", "filteredDF.show(truncate=False)\n", "print(\"Less than 10 years are\", filteredDF.count(), \"people\")"], "outputs": [], "metadata": {}}, {"source": ["(These are some _seriously_ precocious children...)\n", "\n", "# Python lambda functions and User Defined Functions\n", "\n", "In a previous tutorial, you learned to use lambda functions in your Python map \n", "and reduce. You can also apply those with Spark DataFrames as in:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["less_ten_lambda = lambda s: s < 10\n", "lambdaDF = subDF.filter(less_ten_lambda(subDF.age))\n", "lambdaDF.show()\n", "lambdaDF.count()"], "outputs": [], "metadata": {}}, {"source": ["Past versions of DataFrames used to wrap lambdas around\n", "Spark _User Defined Function_ (UDF). A UDF is a special wrapper around a\n", "function, allowing the function to be used in a DataFrame query,\n", "and requires both the function and the return type to be defined."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from pyspark.sql.types import BooleanType\n", "from pyspark.sql.functions import udf\n", "\n", "less_ten = udf(lambda s: s < 10, BooleanType())\n", "lambdaDF = subDF.filter(less_ten(subDF.age))\n", "lambdaDF.show()\n", "lambdaDF.count()"], "outputs": [], "metadata": {}}, {"source": ["Lets try another example below."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # Let's collect the even values less than 10\n", "even = udf(lambda s: s % 2 == 0, BooleanType())\n", "evenDF = lambdaDF.filter(even(lambdaDF.age))\n", "evenDF.show()\n", "evenDF.count()"], "outputs": [], "metadata": {}}, {"source": ["**Exercise**\n", "\n", "You can rewrite some of the examples above, using lambdas!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Transform dataDF through a select transformation and rename the newly created '(age -1)' column to 'age'\n", " \n", " "], "outputs": [], "metadata": {}}, {"source": ["# Additional DataFrame actions\n", "\n", "Let's investigate some additional actions:\n", "\n", "- [`first()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first)\n", "- [`take()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)\n", "\n", "One useful thing to do when we have a new dataset is to look at the first few\n", "entries to obtain a rough idea of what information is available.  In Spark, we\n", "can do that using actions like `first()`, `take()`, and `show()`. Note that for\n", "the `first()` and `take()` actions, the elements that are returned depend on how\n", "the DataFrame is *partitioned*.\n", "\n", "Instead of using the `collect()` action, we can use the `take(n)` action to\n", "return the first _n_ elements of the DataFrame. The `first()` action returns the\n", "first element of a DataFrame, and is equivalent to `take(1)[0]`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(\"first: \", filteredDF.first(), \"\\n\")\n", "\n", "print(\"Four of them: \", filteredDF.take(4))"], "outputs": [], "metadata": {}}, {"source": ["This looks better:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["filteredDF.show(4)"], "outputs": [], "metadata": {}}, {"source": ["## Additional DataFrame transformations\n", "\n", "### _orderBy_\n", "\n", "[`orderBy()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct) \n", "allows you to sort a DataFrame by one or more columns, producing a new DataFrame.\n", "\n", "For example, let's get the first five oldest people in the original (unfiltered)\n", "DataFrame. We can use the `orderBy()` transformation. `orderBy` takes one or\n", "more columns, either as _names_ (strings) or as `Column` objects. To get a\n", "`Column` object, we use one of two notations on the DataFrame:\n", "\n", "- Pandas-style notation: `filteredDF.age`\n", "- Subscript notation: `filteredDF['age']`\n", "\n", "Both of those syntaxes return a `Column`, which has additional methods like\n", "`desc()` (for sorting in descending order) or `asc()` (for sorting in ascending\n", "order, which is the default).\n", "\n", "Here are some examples:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # sort by age in ascending order; \n", " # returns a new DataFrame\n", "dataDF.orderBy(dataDF['age'])  \n", "\n", " # sort by last name in descending order\n", "dataDF.orderBy(dataDF.last_name.desc()) "], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # Get the five oldest people in the list. To do that, sort by age in descending order.\n", "dataDF.orderBy(dataDF.age.desc()).take(5)"], "outputs": [], "metadata": {}}, {"source": ["Or use `show` for nicer printing:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # Get the five oldest people in the list. To do that, sort by age in descending order.\n", "dataDF.orderBy(dataDF.age.desc()).show(5)"], "outputs": [], "metadata": {}}, {"source": ["Note that the results may not the same! Why?\n", "\n", "**Exercise**  \n", "Write down the name of the five oldest people, and then run the cell above `dataDF.orderBy(dataDF.age.desc()).show(5)` again. What do you notice? Explain why.\n", "\n", "**Exercise**  \n", "\n", "Count how many persons have the maximum age."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# your code here\n"], "outputs": [], "metadata": {}}, {"source": ["Let's reverse the sort order. Since ascending sort is the default, we can\n", "actually use a `Column` object expression or a simple string, in this case. The\n", "`desc()` and `asc()` methods are only defined on `Column`. Something like\n", "`orderBy('age'.desc())` would not work, because there's no `desc()` method on\n", "Python string objects. That's why we needed the column expression. But if we're\n", "just using the defaults, we can pass a string column name into `orderBy()`. This\n", "is sometimes easier to read!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.orderBy('age').show(5)"], "outputs": [], "metadata": {}}, {"source": ["### _distinct_ and _dropDuplicates_\n", "\n", "[`distinct()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#p\n", "yspark.sql.DataFrame.distinct) filters out duplicate rows, and it considers all\n", "columns. Since our data is completely randomly generated,\n", "it's extremely unlikely that there are any duplicate rows:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(dataDF.count())\n", "print(dataDF.distinct().count())"], "outputs": [], "metadata": {}}, {"source": ["To demonstrate `distinct()`, let's create a quick throwaway dataset."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["tempDF = sqlContext.createDataFrame([(\"Joe\", 1), (\"Joe\", 1), (\"Anna\", 15), (\"Anna\", 12), (\"Ravi\", 5)], ('name', 'score'))"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["tempDF.show()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["tempDF.distinct().show()"], "outputs": [], "metadata": {}}, {"source": ["Note that one of the (\"Joe\", 1) rows was removed, but both rows with name \"Anna\"\n", "were kept, because all columns in a row must match another row for it to be\n", "considered a duplicate.\n", "\n", "[`dropDuplicates()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates) \n", "is like `distinct()`, except that it\n", "allows us **to specify the columns to compare**. For instance, we can use it to drop\n", "all rows where the first name and last name duplicates (ignoring the occupation\n", "and age columns)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(dataDF.count())\n", "print (dataDF.dropDuplicates(['first_name', 'last_name']).count())"], "outputs": [], "metadata": {}}, {"source": ["Note that with `dropDuplicates` you are not selecting columns! The dataframe still contains all the columns of the original one. Can you verify below?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df1 = dataDF.dropDuplicates(['first_name', 'last_name'])\n", "# print the schema and/or show some rows\n", "\n"], "outputs": [], "metadata": {}}, {"source": ["### _drop_\n", "\n", "[`drop()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop) \n", "is like the opposite of `select()`: Instead of selecting\n", "specific columns from a DataFrame, it drops a specified column from a DataFrame.\n", "\n", "Here's a simple use case: Suppose you're reading from a 1,000-column CSV file,\n", "and you have to get rid of five of the columns. Instead of selecting 995 of the\n", "columns, it's easier just to drop the five you don't want."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.drop('occupation').drop('age').show()"], "outputs": [], "metadata": {}}, {"source": ["### _groupBy_\n", "\n", "[`groupBy()`]((http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#p\n", "yspark.sql.DataFrame.groupBy) is one of the most powerful transformations. It\n", "allows you to perform aggregations on a DataFrame.\n", "\n", "Unlike other DataFrame transformations, `groupBy()` does _not_ return a\n", "DataFrame. Instead, it returns a special [GroupedData](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) \n", "object that contains various aggregation functions.\n", "\n", "The most commonly used aggregation function is [`count()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.count),\n", "but there are others (like [`sum()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.sum), \n", "[`max()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.max), \n", "and [avg()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.avg).\n", "\n", "These aggregation functions typically create a new column and return a new\n", "DataFrame."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.groupBy('occupation').count().show(truncate=False)"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["dataDF.groupBy().avg('age').show(truncate=False)"], "outputs": [], "metadata": {}}, {"source": ["We can also use `groupBy()` to do another useful aggregations:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(\"Maximum age:\", dataDF.groupBy().max('age').first()[0])\n", "print(\"Minimum age:\", dataDF.groupBy().min('age').first()[0])"], "outputs": [], "metadata": {}}, {"source": ["### _sample_\n", "\n", "When analyzing big data, the [`sample()`](http://spark.apache.org/docs/latest/api/py\n", "thon/pyspark.sql.html#pyspark.sql.DataFrame.sample) transformation is often\n", "quite useful. It returns a new DataFrame with a random sample of elements from\n", "the dataset.  It takes in a `withReplacement` argument, which specifies whether\n", "it is okay to randomly pick the same item multiple times from the parent\n", "DataFrame (so when `withReplacement=True`, you can get the same item back\n", "multiple times). It takes in a `fraction` parameter, which specifies the\n", "fraction elements in the dataset you want to return. (So a `fraction` value of\n", "`0.20` returns 20% of the elements in the DataFrame.) It also takes an optional\n", "`seed` parameter that allows you to specify a seed value for the random number\n", "generator, so that reproducible results can be obtained."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["sampledDF = dataDF.sample(withReplacement=False, fraction=0.10)\n", "print(sampledDF.count())\n", "sampledDF.show()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(dataDF.sample(withReplacement=False, fraction=0.05).count())"], "outputs": [], "metadata": {}}, {"source": ["**Question**  \n", "The result of count may seem a bit odd. Can you think why?\n", "\n", "### Data partitioning\n", "\n", "Spark has a dedicated method for splitting randomly a dataset into\n", "partitions."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["(trainData, valData, testData) = dataDF.randomSplit([0.6, 0.2, 0.2])"], "outputs": [], "metadata": {}}, {"source": ["Check the sizes of the three dataframes created below."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(trainData.count())\n", "# do the same for the rest splits\n", "\n"], "outputs": [], "metadata": {}}, {"source": ["What happens? Can you explain why?\n", "\n", "# Caching DataFrames and storage options\n", "\n", "## (a) Caching DataFrames\n", "\n", "For efficiency Spark keeps your DataFrames in memory. (More formally, it keeps\n", "the _RDDs_ that implement your DataFrames in memory.) By keeping the contents in\n", "memory, Spark can quickly access the data. However, memory is limited, so if you\n", "try to keep too many partitions in memory, Spark will automatically delete\n", "partitions from memory to make space for new ones. If you later refer to one of\n", "the deleted partitions, Spark will automatically recreate it for you, but that\n", "takes time.\n", "\n", "So, if you plan to use a DataFrame more than once, then you should tell Spark to\n", "cache it. You can use the `cache()` operation to keep the DataFrame in memory.\n", "However, you must still trigger an action on the DataFrame, such as `collect()`\n", "or `count()` before the caching will occur. In other words, `cache()` is lazy:\n", "It merely tells Spark that the DataFrame should be cached _when the data is\n", "materialized_. You have to run an action to materialize the data; the DataFrame\n", "will be cached as a side effect. The next time you use the DataFrame, Spark will\n", "use the cached data, rather than recomputing the DataFrame from the original\n", "data.\n", "\n", "You can see your cached DataFrame in the \"Storage\" section of the Spark web UI.\n", "If you click on the name value, you can see more information about where the the\n", "DataFrame is stored."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Cache the DataFrame\n", "filteredDF.cache()\n", "# Trigger an action\n", "print(filteredDF.count())\n", "# Check if it is cached\n", "print(filteredDF.is_cached)"], "outputs": [], "metadata": {}}, {"source": ["## (b) Unpersist and storage options\n", "\n", "Spark automatically manages the partitions cached in memory. If it has more\n", "partitions than available memory, by default, it will evict older partitions to\n", "make room for new ones. For efficiency, once you are finished using cached\n", "DataFrame, you can optionally tell Spark to stop caching it in memory by using\n", "the DataFrame's `unpersist()` method to inform Spark that you no longer need the\n", "cached data.\n", "\n", "**Advanced:**  \n", "Spark provides many more options for managing how DataFrames\n", "cached. For instance, you can tell Spark to spill cached partitions to disk when\n", "it runs out of memory, instead of simply throwing old ones away. You can explore\n", "the API for DataFrame's [`persist()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist) \n", "operation using Python's [`help()`](https://docs.python.org/2/library/functions.html?highlight=help#help)\n", "command. The `persist()` operation, optionally, takes a pySpark [StorageLevel](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel)\n", "object."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed\n", "filteredDF.unpersist()\n", " # Check if it is cached\n", "print(filteredDF.is_cached)"], "outputs": [], "metadata": {}}, {"source": ["# Debugging Spark applications and lazy evaluation\n", "\n", "## (a) How Python is Executed in Spark\n", "\n", "Internally, Spark executes using a Java Virtual Machine (JVM). pySpark runs\n", "Python code in a JVM using [Py4J](http://py4j.sourceforge.net). Py4J enables\n", "Python programs running in a Python interpreter to dynamically access Java\n", "objects in a Java Virtual Machine. Methods are called as if the Java objects\n", "resided in the Python interpreter and Java collections can be accessed through\n", "standard Python collection methods. Py4J also enables Java programs to call back\n", "Python objects.\n", "\n", "Because pySpark uses Py4J, coding errors often result in a complicated,\n", "confusing stack trace that can be difficult to understand. In the following\n", "section, we'll explore how to understand stack traces.\n", "\n", "## (b) Challenges with lazy evaluation using transformations and actions\n", "\n", "Spark's use of lazy evaluation can make debugging more difficult because code is\n", "not always executed immediately. \n", "To see an example of how this can happen, let's first define a broken filter function.\n", "Next we perform a `filter()` operation using the broken filtering function.  \n", "No error will occur at this point due to Spark's use of lazy evaluation.\n", "\n", "The `filter()` method will not be executed *until* an action operation is\n", "invoked on the DataFrame.  We will perform an action by using the `count()`\n", "method to return a list that contains all of the elements in this DataFrame."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["def brokenTen(value):\n", "    \"\"\"\n", "    This function raises a NameError, as variable 'val' does not exist; the parameter is \n", "    named 'value'.\n", "    \"\"\"\n", "    if (val < 10):\n", "        return True\n", "    else:\n", "        return False\n", "\n", "btUDF = udf(brokenTen)\n", "brokenDF = subDF.filter(btUDF(subDF.age) == True)"], "outputs": [], "metadata": {}}, {"source": ["To see the error, we need to call an action!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # Now we'll see the error\n", " # Scroll through the message\n", "brokenDF.count()"], "outputs": [], "metadata": {}}, {"source": ["## (c) Finding the bug\n", "\n", "When the `filter()` method is executed, Spark calls the UDF. Since our UDF has\n", "an error in the underlying filtering function `brokenTen()`, an error occurs.\n", "\n", "Scroll through the output \"Py4JJavaError     Traceback (most recent call last)\"\n", "part of the cell and first you will see that the line that generated the error\n", "is the `count()` method line. There is *nothing wrong with this line*. However,\n", "it is an action and that caused other methods to be executed. Continue scrolling\n", "through the Traceback and you will see the following error line:\n", "\n", "`NameError: global name 'val' is not defined`\n", "\n", "Looking at this error line, we can see that we used the wrong variable name in\n", "our filtering function `brokenTen()`.\n", "\n", "## (d) Moving toward expert style\n", "\n", "As you are learning Spark, I recommend that you write your code in the form:\n", "\n", "<pre>    \n", "df2 = df1.transformation1()\n", "df2.action1()\n", "\n", "df3 = df2.transformation2()\n", "df3.action2()\n", "</pre>\n", "\n", "\n", "Using this style will make debugging your code much easier as it makes errors\n", "easier to localize - errors in your transformations will occur when the next\n", "action is executed.\n", "\n", "Once you become more experienced with Spark, you can write your code with the\n", "form: \n", "\n", "<pre>\n", "df.transformation1()\\\n", "  .transformation2()\\\n", "  .action()\n", "</pre>\n", "\n", "\n", "\n", "##  (e) Readability and code style\n", "\n", "Spark style is also to use `lambda` functions instead of separately defined functions,\n", "when their use improves readability and conciseness."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Cleaner code through lambda use\n", "myUDF = udf(lambda v: v < 10)\n", "subDF.filter(myUDF(subDF.age) == True)"], "outputs": [], "metadata": {}}, {"source": ["To make the expert coding style more readable, enclose the statement in\n", "parentheses and put each method, transformation, or action on a separate line."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Final version\n", "from pyspark.sql.functions import *\n", "dataDF.filter(dataDF.age > 20)\\\n", "      .select(concat(dataDF.first_name, lit(' '), dataDF.last_name).alias('full_name'), dataDF.occupation)\\\n", "      .show(10)"], "outputs": [], "metadata": {}}, {"source": ["# Behind the scenes (optional)\n", "\n", "When you use DataFrames or Spark SQL, you are building up a _query plan_. Each\n", "transformation you apply to a DataFrame adds some information to the query plan.\n", "When you finally call an action, which triggers execution of your Spark job,\n", "several things happen:\n", "\n", "1. Spark's Catalyst optimizer analyzes the query plan (called an _unoptimized\n", "logical query plan_) and attempts to optimize it. Optimizations include (but\n", "aren't limited to) rearranging and combining `filter()` operations for\n", "efficiency, converting `Decimal` operations to more efficient long integer\n", "operations, and pushing some operations down into the data source (e.g., a\n", "`filter()` operation might be translated to a SQL `WHERE` clause, if the data\n", "source is a traditional SQL RDBMS). The result of this optimization phase is an\n", "_optimized logical plan_.\n", "2. Once Catalyst has an optimized logical plan, it then constructs multiple\n", "_physical_ plans from it. Specifically, it implements the query in terms of\n", "lower level Spark RDD operations.\n", "3. Catalyst chooses which physical plan to use via _cost optimization_. That is,\n", "it determines which physical plan is the most efficient (or least expensive),\n", "and uses that one.\n", "4. Finally, once the physical RDD execution plan is established, Spark actually\n", "executes the job.\n", "\n", "You can examine the query plan using the `explain()` function on a DataFrame. By\n", "default, `explain()` only shows you the final physical plan; however, if you\n", "pass it an argument of `True`, it will show you all phases.\n", "\n", "(If you want to take a deeper dive into how Catalyst optimizes DataFrame\n", "queries, this blog post, while a little old, is an excellent overview: [Deep\n", "Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13\n", "/deep-dive-into-spark-sqls-catalyst-optimizer.html).)\n", "\n", "Let's add a couple transformations to our DataFrame and look at the query plan\n", "on the resulting transformed DataFrame. Don't be too concerned if it looks like\n", "gibberish. As you gain more experience with Apache Spark, you'll begin to be\n", "able to use `explain()` to help you understand more about your DataFrame\n", "operations."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["newDF = dataDF.distinct().select('*')\n", "newDF.explain(True)"], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python with Pixiedust (Spark 2.2)", "name": "pythonwithpixiedustspark22", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}