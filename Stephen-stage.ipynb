{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pixiedust in c:\\users\\steph\\appdata\\roaming\\python\\python36\\site-packages\n",
      "Requirement already up-to-date: mpld3 in c:\\users\\steph\\anaconda3\\lib\\site-packages (from pixiedust)\n",
      "Requirement already up-to-date: lxml in c:\\users\\steph\\anaconda3\\lib\\site-packages (from pixiedust)\n",
      "Requirement already up-to-date: geojson in c:\\users\\steph\\anaconda3\\lib\\site-packages (from pixiedust)\n",
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.3</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade --user pixiedust\n",
    "import pixiedust\n",
    "# pixiedust.enableSparkJobProgressMonitor()\n",
    "# pixiedust.enableJobMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, log, exp\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# For categorical variables\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "conf = (SparkConf()\n",
    " .setMaster(\"local\")\n",
    " .setAppName(\"Cleaner\")\n",
    " .set(\"spark.executor.memory\", \"1g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lengthInspector(rdd):\n",
    "    \"\"\"Checks rdd for empty rows.\n",
    "    \n",
    "    Key inputs:\n",
    "    rdd --- a Spark rdd.\n",
    "    \"\"\"\n",
    "    lenCounter = rdd.map(lambda x: (len(x), 1))\\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "    lenList = lenCounter.collect()\n",
    "    return lenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMainKey(line):\n",
    "    mainKey = \"Farm \" + line[1].strip() + \" House \" + line[2].strip() + \" Flock \" + line[3].strip()\n",
    "    newline = [mainKey] + line\n",
    "    return newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Do_Machine_Learning(trainingSet, testSet, predictor_cols, dependent_variable, regressor = LinearRegression(),\n",
    "                         paramGrid = [], evalMetric = \"rmse\", seed = None):\n",
    "    \"\"\"\n",
    "    Return (<object> model, <float> error_estimate, <dataframe> result)\n",
    "    \n",
    "    trainingSet: dataframe, used to train model\n",
    "    testSet: dataframe, used to feed the model and get the result(and error estimate)\n",
    "    xCols: list of strings, names of columns which used as inputs\n",
    "    yValues: string, name of column that contains dependent values \n",
    "    regressor: Regression object, by default = LinearRegression()\n",
    "    paramGrid: list built byParamGridBuilder, by default = empty list\n",
    "    evalMetric: string, name of matrix used for evaluation, by default = \"rmse\"\n",
    "    seed: int or None, seed for random number generator, if == None will use random numbers\n",
    "    \n",
    "    !!! seed is useless at that time !!!\n",
    "    \"\"\"\n",
    "    # push estimator into pipeline\n",
    "    vec = VectorAssembler(inputCols = predictor_cols, outputCol = \"features\")\n",
    "    regPipeline = Pipeline()\n",
    "    regPipeline.setStages([vec, regressor])   \n",
    "    # build evaluator\n",
    "    regEval = RegressionEvaluator(predictionCol = \"Predicted_\"+dependent_variable, labelCol = dependent_variable, \n",
    "                                  metricName = evalMetric)\n",
    "    # combine estimator and evaluator to a cross validator\n",
    "    crossval = CrossValidator(estimator = regPipeline, evaluator = regEval, numFolds = 3)\n",
    "    # set parameters grid\n",
    "    crossval.setEstimatorParamMaps(paramGrid)\n",
    "    # trainning\n",
    "    regModel = crossval.fit(trainingSet).bestModel\n",
    "    # predicting\n",
    "    predictions = regModel.transform(testSet)\n",
    "    # get evaluating result\n",
    "    evaluation = regEval.evaluate(predictions)\n",
    "    \n",
    "    return regModel, evaluation, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the csv file to a tab delimited file -> makes life easier\n",
    "with open('FF_broilers_v2.csv', 'r') as fin:\n",
    "    with open('FF_broilers_v2_tab.txt', 'w') as fout:\n",
    "        reader = csv.DictReader(fin)\n",
    "        writer = csv.DictWriter(fout, reader.fieldnames, delimiter='\\t')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of length of lines in this file: \n",
      "[(38, 3678), (1, 3679)]\n"
     ]
    }
   ],
   "source": [
    "# Read in the data in tab-delimited form\n",
    "rdd = sc.textFile('FF_broilers_v2_tab.txt')\n",
    "\n",
    "# Initialise the header for creating the dataframe\n",
    "header = []\n",
    "\n",
    "data = rdd.map(lambda x: x.split('\\t'))\n",
    "for col in data.take(1)[0]:\n",
    "    header.append(col.strip()\n",
    "                     .replace(' ', '')\n",
    "                     .replace('#', '')\n",
    "                     .replace('(', '')\n",
    "                     .replace(')', '')\n",
    "                     .replace('.', '')\n",
    "                     .replace('/', 'Per')\n",
    "                     .replace('%', 'Percentage'))\n",
    "    \n",
    "lenList = lengthInspector(data)\n",
    "print(\"Distribution of length of lines in this file: \")\n",
    "print(lenList)\n",
    "\n",
    "data_clean = data.filter(lambda x: len(x) >= 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the header from the rdd\n",
    "rdd_header = data_clean.take(1)[0]\n",
    "rdd_rows = data_clean.filter(lambda line: line != rdd_header)\n",
    "\n",
    "broilersDF = sqlContext.createDataFrame(rdd_rows, header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe of only the relevant columns\n",
    "desired_cols = ['CustomerCode', 'Flock', 'House', 'GeneticLineCode', 'BirdsPresent', 'Mortality', 'BodyWeightg', 'DailyGaing', \n",
    "               'WheatPerBird', 'FeedIntakePerBirdg', 'WaterIntakePerBirdml']\n",
    "\n",
    "df_new = broilersDF.select([c for c in broilersDF.columns if c in desired_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any white space to the left or right of any entries\n",
    "for col in df_new.schema.names:\n",
    "    df_new = df_new.withColumn(col, ltrim(df_new[col]))\n",
    "    df_new = df_new.withColumn(col, rtrim(df_new[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to remove all spaces from a column's entries\n",
    "spaceDeleteUDF = udf(lambda s: s.replace(\" \", \"\"), StringType())\n",
    "\n",
    "\n",
    "# Designate columns that are incorrectly encoded as strings\n",
    "string_cols_incorrect = ['BirdsPresent', 'Mortality', 'BodyWeightg', 'DailyGaing', 'WheatPerBird',\n",
    "                         'FeedIntakePerBirdg', 'WaterIntakePerBirdml']\n",
    "\n",
    "# First clean the columns of spaces and then convert to Doubles\n",
    "for col in string_cols_incorrect:\n",
    "    df_new = df_new.withColumn(col, spaceDeleteUDF(col))\n",
    "    df_new = df_new.withColumn(col, df_new[col].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['GeneticLineCode', 'CustomerCode', 'House']\n",
    "\n",
    "##=== build stages ======\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) \n",
    "                        for c in categorical_columns]\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "\n",
    "## build pipeline model\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "## fit pipeline model\n",
    "pipeline_mode = pipeline.fit(df_new)\n",
    "\n",
    "## transform data\n",
    "df_coded = pipeline_mode.transform(df_new)\n",
    "\n",
    "## remove uncoded columns\n",
    "# selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['x4', 'y1', 'y2']\n",
    "cols = df_coded.schema.names\n",
    "cols_to_keep = [col for col in cols if col not in categorical_columns]\n",
    "df_coded = df_coded.select(cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows containing NA\n",
    "df_coded = df_coded.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalise feed, water and wheat per bird wrt bird body weight\n",
    "# Note that this also reduces the comlexity of physical dimensions (reverting to ratios of SI)\n",
    "df_new = df_coded.withColumn(\"Feed_%_body_weight\", df_coded['FeedIntakePerBirdg']/df_coded['BodyWeightg'])\n",
    "df_new = df_new.withColumn(\"Water_%_body_weight\", df_coded['WaterIntakePerBirdml']/df_coded['BodyWeightg'])\n",
    "df_new = df_new.withColumn(\"Wheat_%_body_weight\", df_coded['WheatPerBird']/df_coded['BodyWeightg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and training data\n",
    "train, test = df_new.randomSplit([8.0, 2.0], 940309160050)\n",
    "train = train.cache()\n",
    "test = test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Flock', 'BirdsPresent', 'BodyWeightg', 'DailyGaing', 'WheatPerBird', 'FeedIntakePerBirdg', 'WaterIntakePerBirdml', 'stringindexed_GeneticLineCode', 'stringindexed_CustomerCode', 'stringindexed_House', 'onehotencoded_GeneticLineCode', 'onehotencoded_CustomerCode', 'onehotencoded_House', 'Feed_%_body_weight', 'Water_%_body_weight', 'Wheat_%_body_weight']\n"
     ]
    }
   ],
   "source": [
    "list(itertools.permutations([1, 2, 3]))\n",
    "\n",
    "dependent_variable = 'Mortality'\n",
    "predictors = [variable for variable in df_new.schema.names if variable != dependent_variable]\n",
    "print(predictors)\n",
    "\n",
    "# Want to have permutations such that Feed and Feed % body weight do not occur in the same list - true for all variaitons\n",
    "# Also want stringindexed_ and onehotencoded_ variables together always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build regressor\n",
    "lr = LinearRegression()\n",
    "lr.setPredictionCol(\"Predicted Mortality\")\\\n",
    "   .setLabelCol(\"Death\")\n",
    "\n",
    "# build parameter grid\n",
    "regParam = [x / 100.0 for x in range(1, 10)]\n",
    "pg = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, regParam)\n",
    "             .build())\n",
    "\n",
    "import itertools\n",
    "dependent_variable = 'Mortality'\n",
    "predictors = [variable for variable in df_new.schema.names if variable != dependent_variable]\n",
    "\n",
    "predictors = list(itertools.permutations([1, 2, 3]))\n",
    "\n",
    "# run ML\n",
    "model1, result1, predictionDF1 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"ln\"], \"Death\", lr, pg)\n",
    "model2, result2, predictionDF2 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"square\"], \"Death\", lr, pg)\n",
    "model3, result3, predictionDF3 = Do_Machine_Learning(trainDF, testDF, [\"Age\", \"exp\"], \"Death\", lr, pg)\n",
    "\n",
    "\n",
    "# print attributions of model\n",
    "\"\"\"\n",
    "print(\"attributes of the model are: {}\".format(dir(model)))\n",
    "print(\"method list: {}\".format([method for method in dir(model) if callable(getattr(model, method))]))\n",
    "print(model.stages)\n",
    "\"\"\"\n",
    "\n",
    "# Print coefficients and intercept\n",
    "weights1 = model1.stages[1].coefficients\n",
    "ic1 = model1.stages[1].intercept\n",
    "weights2 = model2.stages[1].coefficients\n",
    "ic2 = model2.stages[1].intercept\n",
    "weights3 = model3.stages[1].coefficients\n",
    "ic3 = model3.stages[1].intercept\n",
    "print(weights1, weights2, weights3)\n",
    "print(ic1, ic2, ic3)\n",
    "#print(list(zip([\"Age\"], weights1)))\n",
    "#print(model.stages[1].intercept)\n",
    "\n",
    "# print error and result\n",
    "print(\"Mean Squared Error: {0:2.2f}, {1:2.2f}, {2:2.2f}\\n\".format(result1, result2, result3))\n",
    "predictionDF1.show()\n",
    "predictionDF2.show()\n",
    "predictionDF3.show()\n",
    "\n",
    "# print the model\n",
    "# print(model.stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
