{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["# Non Linear Regression modelling\n", "\n", "In this tutorial, we'll recap Pipelines from the previous one, and develop and evaluate two more regression modeling algrithms, from the ML pyspark module:\n", "\n", "- [Decision Tree Regression](https://spark.apache.org/docs/2.0.0/api/python/pysp\n", "ark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) and\n", "- [Random Forest Regression](https://spark.apache.org/docs/2.0.0/api/python/pysp\n", "ark.ml.html#pyspark.ml.regression.RandomForestRegressor)\n", "\n", "Remember that regression models predict a numeric value from a vector of input\n", "variables.\n", "\n", "In the Power Plant example we will use the two methods above to create models\n", "that predict PE (Power output) from the rest four:\n", "- AT = Atmospheric Temperature in C\n", "- V = Exhaust Vacuum Speed\n", "- AP = Atmospheric Pressure\n", "- RH = Relative Humidity\n", "- PE = Power Output.  This is the value we are trying to predict given the\n", "measurements above.\n", "\n", "# Data preparation\n", "First, lets initialize the Spark environment with the following code:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import pixiedust"], "outputs": [], "metadata": {}}, {"source": ["To enable monitoring of Spark via the notebook:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["pixiedust.enableJobMonitor()"], "outputs": [], "metadata": {}}, {"source": ["Now there is a Spark Session, named `spark` that available for this notebook. Check it out  here:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["spark"], "outputs": [], "metadata": {}}, {"source": ["## Preprocessing Step 1: Loading data"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["powerPlantDF = spark.read.csv('../data/powerplant/', header=True, inferSchema = True)"], "outputs": [], "metadata": {}}, {"source": ["## Preprocessing Step 2: Data splitting\n", "\n", "Lets follow the same train-test split as we did in the previous tutorial."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["seed = 1800009193\n", "\n", "(split20DF, split80DF) = datasetDF.randomSplit([0.2,0.8],seed)\n", "\n", "testSetDF = split20DF.cache()\n", "trainingSetDF = split80DF.cache()"], "outputs": [], "metadata": {}}, {"source": ["## Step 3: Prepare features\n", "Initialize the VectorAssembler to  extract the input variables as features."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from pyspark.ml.feature import VectorAssembler\n", "vectorizer = VectorAssembler(\n", "    inputCols=[\"AT\", \"V\", \"AP\", \"RH\"],\n", "    outputCol=\"features\")"], "outputs": [], "metadata": {}}, {"source": ["# Part 1: Decision Tree Regression\n", "\n", "[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning)\n", "uses a [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) as a\n", "predictive model which maps observations about an item to conclusions about the\n", "item's target value. It is one of the predictive modelling approaches used in\n", "statistics, data mining and machine learning. Decision trees where the target\n", "variable can take continuous values (typically real numbers) are called\n", "regression trees.\n", "\n", "Spark ML Pipeline provides [DecisionTreeRegressor()](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)\n", "as an implementation of Decision Tree\n", "Learning\n", "\n", "The cell below we reproduce the learning steps as above, but now  based on the\n", "Spark ML Pipeline API for Decision Tree Regressor.\n", "\n", "\n", "\n", "### Challenge 1\n", "\n", "Create a Decision Tree regressor for estimating the powerplant energy production!\n", "\n", "- In the next cell, we create a DecisionTreeRegressor\n", "and set the parameters for the method:\n", "  - Set the name of the prediction column to \"Predicted_PE\"\n", "  - Set the name of the features column to \"features\"\n", "  - Set the maximum depth of the tree  to 3\n", "- Create the ML Pipeline and set the stages to the Vectorizer we created\n", "earlier and DecisionTreeRegressor learner we just created.\n", "\n", "Check the [Decision Tree Regressor](https://spark.apache.org/docs/2.0.0/api/pyt\n", "hon/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) documentation, if needed."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["from pyspark.ml.regression import DecisionTreeRegressor\n", "dt = DecisionTreeRegressor()\n", "dt.setPredictionCol(\"Predicted_PE\")\\\n", "  .setLabelCol(\"PE\")\\\n", "  .setMaxDepth(3)\n", "\n", "\n", "dtPipeline = Pipeline()\n", "dtPipeline.setStages([vectorizer, dt])"], "outputs": [], "metadata": {}}, {"source": ["### Challenge 2\n", "\n", "Instead of guessing what parameters to use, employ _Model Selection_ to find the best model.\n", "\n", "Reuse the CrossValidator of the previous Tutorial by replacing the\n", "Estimator with our new `dtPipeline`. Keep the rest of the parameters the same i.e. the number of folds remains 3.\n", "\n", "- Build a parameter grid with the\n", "parameter `dt.maxDepth` and a list of the values 2 and 3, and add the grid to\n", "the CrossValidator\n", "- Run the CrossValidator to find the parameters that yield\n", "the best model (i.e. lowest RMSE) and return the best model.\n", "\n", "_Note that it will take some time to run the [CrossValidator](https://spark.apac\n", "he.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n", "as it will run almost 50 Spark jobs_"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Your code here"], "outputs": [], "metadata": {}}, {"source": ["Now let's see how our tuned DecisionTreeRegressor model's RMSE and \\\\(r^2\\\\)\n", "values compare to our tuned LinearRegression model.\n", "\n", "Write the code to calculate and print the predictions of the best decision tree model."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Your code here"], "outputs": [], "metadata": {}}, {"source": ["# Random Forest regression\n", "[Random forests](https://en.wikipedia.org/wiki/Random_forest) or random decision\n", "tree forests are an ensemble learning method for regression that operate by\n", "constructing a multitude of decision trees at training time and outputting the\n", "class that is the mean prediction (regression) of the individual trees. Random\n", "decision forests correct for decision trees' habit of overfitting to their\n", "training set.\n", "\n", "Spark ML Pipeline provides [RandomForestRegressor()](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor).\n", "\n", "### Challenge 3\n", "\n", "- In the next cell, create a [RandomForestRegressor()](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor)\n", "- The next step is to set the parameters for the method :\n", "  - Set the name of the prediction column to \"Predicted_PE\"\n", "  - Set the name of the features column to \"features\"\n", "  - Set the random number generator seed to 100088121\n", "  - Set the maximum depth to 8\n", "  - Set the number of trees to 30\n", "- Create the ML Pipeline and set the stages to the Vectorizer we created\n", "earlier and RandomForestRegressor() learner we just created.\n", "- Build a parameter grid with the parameter `maxBins ` and a list of the values 50 and 100, and add the grid to\n", "the CrossValidator \n", "- Run the CrossValidator to find the parameters that yield\n", "the best model (i.e. lowest RMSE) and return the best model."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Create a RandomForestRegressor\n", "from pyspark.ml.regression import RandomForestRegressor\n", "\n", "rf = RandomForestRegressor()\n", "\n", "rf.setLabelCol(\"PE\")\\\n", "  .setPredictionCol(\"Predicted_PE\")\\\n", "  .setFeaturesCol(\"features\")\\\n", "  .setSeed(100088121)\\\n", "  .setMaxDepth(8)\\\n", "  .setNumTrees(30)\n", "\n", "# Your code here\n", "  "], "outputs": [], "metadata": {}}, {"source": ["Now let's see how our tuned RandomForestRegressor model's RMSE and \\\\(r^2\\\\)\n", "values compare to our tuned LinearRegression and Decision Tree models.\n", "\n", "Write the code to calculate and print the predictions of the best decision tree model."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Your code here"], "outputs": [], "metadata": {}}, {"source": ["Note: Inspecting the best random Forest:  \n", "The line below will pull the Random Forest model named `rfModel` from the Pipeline and display\n", "it.\n", "\n", "\n", "<pre>\n", "print(rfModel.stages[1]._java_obj.toDebugString())\n", "</pre>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Your code here"], "outputs": [], "metadata": {}}, {"source": ["**Discussion**\n", "\n", "How do the r^2 and RMSE values compare for the three models? Which model would you select?\n", "\n", "\n", "\n", "# What comes next?\n", "For your project work we may need other components of the ML module, for\n", "example:\n", "\n", "- [Principal Components Analysis](https://spark.apache.org/docs/2.0.0/api/python\n", "/pyspark.ml.html#pyspark.ml.feature.PCA)\n", "- [Clustering](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#module-pyspark.ml.clustering)\n", "   - [K-Means](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.clustering.KMeans)\n", "   - [Gaussian Mixture Models](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.clustering.GaussianMixture)\n", "- [Classification](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#module-pyspark.ml.classification)\n", "   - [Decision Trees Classifier](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.classification.DecisionTreeClassifier)\n", "   - [Bayesian models](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.classification.NaiveBayes)\n", "   - [Neural networks](https://spark.apache.org/docs/2.0.0/api/python/pyspark.ml.html#pyspark.ml.classification.MultilayerPerceptronClassifier)\n", "\n", "Consult the ML package documentation for those, they follow the same logic as\n", "Regression.\n", "_If you need extra help, ask [Ioannis](mailto:ioannis.athanasiadis@wur.nl)_."], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python with Pixiedust (Spark 2.2)", "name": "pythonwithpixiedustspark22", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}